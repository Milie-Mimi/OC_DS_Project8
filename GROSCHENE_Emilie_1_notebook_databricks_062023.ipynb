{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "455f359e-79b7-4f8e-9a4e-394b1d5dcada",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Projet 8: Déployez un modèle dans le cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e3c6aae-f0ec-442c-88e3-6f6e8778f832",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Fruits est une start-up de l'agri-tech qui a pour volonté de préserver la **biodiversité des fruits** en développant des **robots cueilleurs intelligents** qui appliqueraient des **traitements spécifiques à chaque espèce** de fruits lors de la récolte.\n",
    "\n",
    "Pour se faire connaître auprès du grand public, elle souhaite mettre à sa disposition une **application mobile** qui permettrait aux utilisateurs de **prendre en photo** un fruit et d'obtenir des **informations** sur ce dernier. Trois principaux objectifs:\n",
    "- sensibiliser le grand public à la biodiversité des fruits\n",
    "- mettre en place une première version du moteur de classification des images de fruits\n",
    "- construire une première version de l'architecture Big Data nécessaire\n",
    "\n",
    "**Contraintes à prendre en compte:**\n",
    "- le **volume des données va augmenter très rapidement** après la livraison de ce projet => architecture Big Data avec scripts en Pyspark\n",
    "- compléter le script d'un **traitement de diffusion des poids du modèle Tensorflow sur les clusters**\n",
    "- compléter le script d'une étape de **réduction de dimension de type PCA** en PySpark\n",
    "- les **serveurs** doivent être situés sur le **territoire européen**\n",
    "- faire un **retour critique** sur le choix de l'architecture Big Data choisie\n",
    "\n",
    "Ce notebook contient les **scripts en PySpark exécutables** sur **Databricks** c'est à dire les **étapes de preprocessing** et la **réduction de dimension** de type PCA (Principal Component Analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "121dd62a-7ebf-4fb4-8b36-756eda45d04f",
     "showTitle": false,
     "title": ""
    },
    "heading_collapsed": true
   },
   "source": [
    "## Introduction au Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae6ecb0c-20e6-4e01-bb5d-d9cee18ae43c",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "Le **web 3.0** est celui de la **mobilité**, des **objets connectés** et des **données**. C'est un web sémantique où l'internaute est \"fiché\", notamment au travers de sa navigation et de ses différents profils sur les réseaux sociaux. \n",
    "\n",
    "L'**utilisation massive** d'internet et des objets connectés comme le téléphone, l'ordinateur, la montre, la balance etc engendre la **production de quantités astronomiques de données** ce qui pose des **problématiques de stockage et de traitement approprié** pour les entreprises qui les exploitent afin de prendre notamment des décisions stratégiques et concurrentielles.\n",
    "\n",
    "Cette infographie offre une vision très parlante du monde de la donnée aujourd'hui:\n",
    "\n",
    "![Data](files/tables/img/data_infographie.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5ba6bfa-e8b6-46ce-956b-34861454f15d",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "A titre d'exemple, en 2009, 800 000 pétaoctets de données étaient produits, 35 zettaoctets en 2020 et plus de 180 zettaoctets de données prévus d'ici 2025 (*5 en 5 ans!).\n",
    "\n",
    "Il faut donc des **technologies innovantes** capables de traiter:\n",
    "- des volumes de données énormes et en constante augmentation\n",
    "- des données provenant de sources multiples et de natures diverses\n",
    "- des besoins analytiques vitaux à fournir dans des délais impartis\n",
    "\n",
    "Le Big Data désigne le courant technologique que nous voyons émerger ces dernières années autour des données, des **mégadonnées que nous permettent de stocker aujourd’hui les serveurs**. Le Big Data vient du fait que les données de certaines entreprises ou institutions sont devenues tellement volumineuses que les outils techniques classiques de gestion, de requête sur les bases dites structurées et de traitement des données sont devenus obsolètes, avec des difficultés dans l’instanciation de celles-ci, les temps d’extraction, de traitement devenant trop long.\n",
    "\n",
    "Le socle commun sur lequel à peu près tout le monde s’entend pour caractériser les problématiques de Big Data, ce sont les 4V : Volume, Vitesse, Variété et Véracité.\n",
    "- **Volume** : des volumes de données énormes en constante augmentation\n",
    "- **Vitesse** : des besoins analytiques importants à fournir dans des délais impartis\n",
    "- **Varieté** : des données provenant de sources multiples et de natures diverses\n",
    "- **Véracité** : fait référence à la fiabilité de la donnée, la qualité et la précision sont moins vérifiables\n",
    "\n",
    "Le choix de l'**architecture de données** à mettre en place est donc essentiel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c8d6a9b-ee9e-4d65-a4f9-1a4b2600c854",
     "showTitle": false,
     "title": ""
    },
    "heading_collapsed": true
   },
   "source": [
    "## Choix d'utiliser Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5d7472e-e927-4ab5-a921-f85b0790dfad",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "Plusieurs raisons m'ont poussé à transférer les traitements sur Databricks qui est un environnement SaaS (Software-as-a-Service) permettant d'accéder aux données et aux ressources de calcul:\n",
    "\n",
    "![Databricks](files/tables/img/databricks_logo.PNG)\n",
    "\n",
    "\n",
    "- **simplicité**: Databricks permet une seule architecture de données unifiée sur S3 (solution de stockage des données sur Amazon) pour l'analytique SQL, la data science et le machine learning\n",
    "- **rapport performance / prix**: performances du data warehouse au prix d'un data lake grâce à des clusters de calcul optimisés par SQL\n",
    "- **réputation**: des clients prestigieux comme HP, Nasdac, Hotels.com ont mis en œuvre Databricks sur AWS pour mettre à disposition une plateforme d'analytique révolutionnaire répondant à tous les cas d'usage de l'analytique et de l'IA.\n",
    "\n",
    "Construire des modèles de ML est difficile et les mettre en production est encore plus difficile:\n",
    "- la **diversité des frameworks de ML** complique la gestion des environnements de ML\n",
    "- les **transferts entre équipes sont difficile**s en raison de la disparité des outils et des processus, de la préparation des données à l'expérimentation et à la production\n",
    "- la difficulté de suivre les expériences, les modèles, les dépendances et les artefacts rend **difficile la reproduction des résultats**\n",
    "- il y a des **risques liés à la sécurité et à la conformité**\n",
    "\n",
    "Le maintien de la qualité des données et de la précision des modèles au fil du temps ne sont que quelques-uns des défis à relever. Databricks va **rationaliser le développement ML**, de la préparation des données à l'entraînement et au déploiement des modèles, à **grande échelle** tout en permettant la collaboration:\n",
    "- **workspace**: un lieu central pour stocker et partager des notebooks, des expériences et des projets, avec un contrôle d'accès basé sur les rôles\n",
    "- **notebooks** collaboratifs: prise en charge de plusieurs langues (R, Python, SQL, Scala) et le versionning intégré permettent aux équipes de partager et d'itérer sur le code plus rapidement\n",
    "- **AutoML**: obtention de résultats plus rapides grâce à l'ajustement automatique des hyperparamètres et à la recherche de modèles avec les intégrations Hyperopt, Apache SparkTM et MLflow\n",
    "- **Experiments Tracking**: suivre automatiquement les expériences et utiliser les visualisations intégrées pour voir et comparer les résultats de milliers d'essais, sélectionner les paramètres, les mesures et identifier le meilleur essai.\n",
    "- **Model Registry**: enregistrer le modèle à mettre en production dans un seul endroit et gérer son cycle de vie de manière collaborative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0187c51b-76e7-4ae3-a22a-6462d945b4d7",
     "showTitle": false,
     "title": ""
    },
    "heading_collapsed": true
   },
   "source": [
    "## Monter un bucket AWS S3 sur Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c3a0c97-0488-4991-ad37-e49385881aa8",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "**L'analyse des données directement dans un entrepôt de données peut s'avérer coûteuse**, c'est pourquoi les entreprises recherchent d'autres plateformes capables de stocker et de traiter tout ou partie de leurs données. \n",
    "\n",
    "L'un de ces outils est **Amazon S3** qui est un service en ligne qui offre un **stockage flexible aux entreprises**. Son contrôle d'accès granulaire, son chargement de métadonnées et d'autres caractéristiques de ce type en font le premier choix de tous les analystes de données. Aujourd'hui, les entreprises **transfèrent des informations de Databricks vers S3 afin d'utiliser un espace de stockage évolutif à un prix inférieur**.\n",
    "\n",
    "Pour monter un bucket AWS S3 sur Databricks, se référer au notebook suivant:\n",
    "- Mount_AWS_S3_Bucket.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8ee0d9b-5758-4954-bd54-cf945fd47475",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36b8bb8-4b30-4c41-97ec-009d98b38d03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8534acc-6885-4aaa-8d16-2a6ec7b93637",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Preprocessing des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea3d0796-a2b2-4d49-b4d4-97f1538ddd6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1890876-9ab0-4318-a773-5fbcac36599b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dans cette partie, j'ai enregistré sur l'espace de stockage de Databricks quelques dossiers de fruits comportants chacun 5 images afin de tester le script sur un jeu de données réduit dans un premier temps. J'avais uploadé des images sur Amazon S3 dans un premier temps mais cela revenait cher. Je profite de l'essai gratuit de Databricks pendant 14 jours pour l'élaboration du script avant de l'appliquer sur les données présentes sur S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c57870-f72d-4eff-8f39-2a3869bc3832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dossiers présents dans FileStore\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23dfa0d5-da75-4f86-a94d-284d6d4ac654",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Vérification du dossier où sont stockées les images\n",
    "dbutils.fs.ls(\"/FileStore/tables/data/test_S3/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e100fce-2efe-40c3-a58e-686beaa5b146",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Les différentes vues du fruit sont présentes dans des dossiers qui portent le nom du fruit. Nous allons **charger les données avec une extension.jpg**. \\\n",
    "Databricks recommande d'utiliser la source de données **fichier binaire** pour charger les données d'image dans le DataFrame Spark sous forme d'octets bruts.\\\n",
    "https://docs.databricks.com/external-data/image.html\n",
    "\n",
    "Databricks prend en charge les fichiers binaires et convertit chacun d’eux en un enregistrement unique qui contient le **contenu brut et les métadonnées du fichier**. La source de données de fichier binaire produit un DataFrame avec les colonnes suivantes et éventuellement des colonnes de partition :\n",
    "- **path (StringType)** : Chemin d'accès au fichier.\n",
    "- **modificationTime (TimestampType)** : Heure de dernière modification de l'événement. Dans certaines implémentations Hadoop FileSystem, ce paramètre peut ne pas être disponible et la valeur est définie sur une valeur par défaut.\n",
    "- **length (LongType)** : Longueur du fichier en octets.\n",
    "- **content (BinaryType)** : Contenu du fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b10567-b678-42d8-aad5-35c2399e3be6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Chemin d'accès des données\n",
    "path_data = \"/FileStore/tables/data/test_S3/\"\n",
    "\n",
    "# lecture des fichiers jpg de manière récursive à partir du répertoire \n",
    "# d'entrée en ignorant la détection de la partition\n",
    "images = spark.read.format(\"binaryFile\") \\ # lire les fichiers binaires\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\ # lit tous les fichiers JPG du répertoire d’entrée\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\ # recherche de manière récursive le fichiers\n",
    "  .load(path_data) # chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8fa749e-1a25-4bbe-91f5-1a11ad950721",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5 premières lignes du dataframe issu de la source de données de fichier binaire\n",
    "images.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09fd86fb-08f8-431a-b2bb-3ba2e29c1912",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure image path\n",
    "sample_img_dir = \"/FileStore/tables/data/test_S3/Apple Braeburn/\"\n",
    "\n",
    "# Chargement des images\n",
    "df_img = spark.read.format(\"image\").load(sample_img_dir)\n",
    "\n",
    "# Affichage des images\n",
    "display(df_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32535ae3-f1f3-41d7-8c47-22be53d79258",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Testons l'affichage des photos du premier dossier.\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0ecdfc7-e68d-4247-af6c-6f29eb7f3ff3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Le message d'erreur nous indique un chemin d'accès erroné: l'espace dans le nom du dossier a été remplacé par des caractères spéciaux. Nous allons dans un premier temps remplacer tous les espaces dans le nom des dossiers par un underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e9bd66f-86b3-47eb-ad52-813f7072408d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ea62f2-b966-4221-9702-c461a7152336",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25c1227-7589-440c-bc0a-e28440341d1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/FileStore/tables/data/test_S3/\"\n",
    "for foldername in os.listdir(path):\n",
    "    print(filename)\n",
    "    #os.rename(os.path.join(path,filename),os.path.join(path, filename.replace(' ', '_').lower())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c924b1-1ec6-4b44-aa6c-b2f27d6aabab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ea4263a-2a7a-4c73-afc0-109e9ea44617",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GROSCHENE_Emilie_1_notebook_databricks_062023",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "projet8",
   "language": "python",
   "name": "projet8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
