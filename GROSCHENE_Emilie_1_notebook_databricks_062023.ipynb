{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "455f359e-79b7-4f8e-9a4e-394b1d5dcada",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Projet 8: Déployez un modèle dans le cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e3c6aae-f0ec-442c-88e3-6f6e8778f832",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Fruits est une start-up de l'agri-tech qui a pour volonté de préserver la **biodiversité des fruits** en développant des **robots cueilleurs intelligents** qui appliqueraient des **traitements spécifiques à chaque espèce** de fruits lors de la récolte.\n",
    "\n",
    "Pour se faire connaître auprès du grand public, elle souhaite mettre à sa disposition une **application mobile** qui permettrait aux utilisateurs de **prendre en photo** un fruit et d'obtenir des **informations** sur ce dernier. Trois principaux objectifs:\n",
    "- sensibiliser le grand public à la biodiversité des fruits\n",
    "- mettre en place une première version du moteur de classification des images de fruits\n",
    "- construire une première version de l'architecture Big Data nécessaire\n",
    "\n",
    "**Contraintes à prendre en compte:**\n",
    "- le **volume des données va augmenter très rapidement** après la livraison de ce projet => architecture Big Data avec scripts en Pyspark\n",
    "- compléter le script d'un **traitement de diffusion des poids du modèle Tensorflow sur les clusters**\n",
    "- compléter le script d'une étape de **réduction de dimension de type PCA** en PySpark\n",
    "- les **serveurs** doivent être situés sur le **territoire européen**\n",
    "- faire un **retour critique** sur le choix de l'architecture Big Data choisie\n",
    "\n",
    "Ce notebook contient les **scripts en PySpark exécutables** sur **Databricks** c'est à dire les **étapes de preprocessing** et la **réduction de dimension** de type PCA (Principal Component Analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121dd62a-7ebf-4fb4-8b36-756eda45d04f",
     "showTitle": false,
     "title": ""
    },
    "heading_collapsed": true
   },
   "source": [
    "## Introduction au Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6ecb0c-20e6-4e01-bb5d-d9cee18ae43c",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "Le **web 3.0** est celui de la **mobilité**, des **objets connectés** et des **données**. C'est un web sémantique où l'internaute est \"fiché\", notamment au travers de sa navigation et de ses différents profils sur les réseaux sociaux. \n",
    "\n",
    "L'**utilisation massive** d'internet et des objets connectés comme le téléphone, l'ordinateur, la montre, la balance etc engendre la **production de quantités astronomiques de données** ce qui pose des **problématiques de stockage et de traitement approprié** pour les entreprises qui les exploitent afin de prendre notamment des décisions stratégiques et concurrentielles.\n",
    "\n",
    "Cette infographie offre une vision très parlante du monde de la donnée aujourd'hui:\n",
    "\n",
    "![Data](files/tables/img/data_infographie.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ba6bfa-e8b6-46ce-956b-34861454f15d",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "A titre d'exemple, en 2009, 800 000 pétaoctets de données étaient produits, 35 zettaoctets en 2020 et plus de 180 zettaoctets de données prévus d'ici 2025 (*5 en 5 ans!).\n",
    "\n",
    "Il faut donc des **technologies innovantes** capables de traiter:\n",
    "- des volumes de données énormes et en constante augmentation\n",
    "- des données provenant de sources multiples et de natures diverses\n",
    "- des besoins analytiques vitaux à fournir dans des délais impartis\n",
    "\n",
    "Le Big Data désigne le courant technologique que nous voyons émerger ces dernières années autour des données, des **mégadonnées que nous permettent de stocker aujourd’hui les serveurs**. Le Big Data vient du fait que les données de certaines entreprises ou institutions sont devenues tellement volumineuses que les outils techniques classiques de gestion, de requête sur les bases dites structurées et de traitement des données sont devenus obsolètes, avec des difficultés dans l’instanciation de celles-ci, les temps d’extraction, de traitement devenant trop long.\n",
    "\n",
    "Le socle commun sur lequel à peu près tout le monde s’entend pour caractériser les problématiques de Big Data, ce sont les 4V : Volume, Vitesse, Variété et Véracité.\n",
    "- **Volume** : des volumes de données énormes en constante augmentation\n",
    "- **Vitesse** : des besoins analytiques importants à fournir dans des délais impartis\n",
    "- **Varieté** : des données provenant de sources multiples et de natures diverses\n",
    "- **Véracité** : fait référence à la fiabilité de la donnée, la qualité et la précision sont moins vérifiables\n",
    "\n",
    "Le choix de l'**architecture de données** à mettre en place est donc essentiel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8d6a9b-ee9e-4d65-a4f9-1a4b2600c854",
     "showTitle": false,
     "title": ""
    },
    "heading_collapsed": true
   },
   "source": [
    "## Choix d'utiliser Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d7472e-e927-4ab5-a921-f85b0790dfad",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "Plusieurs raisons m'ont poussé à transférer les traitements sur Databricks qui est un environnement SaaS (Software-as-a-Service) permettant d'accéder aux données et aux ressources de calcul:\n",
    "\n",
    "![Databricks](files/tables/img/databricks_logo.PNG)\n",
    "\n",
    "\n",
    "- **simplicité**: Databricks permet une seule architecture de données unifiée sur S3 (solution de stockage des données sur Amazon) pour l'analytique SQL, la data science et le machine learning\n",
    "- **rapport performance / prix**: performances du data warehouse au prix d'un data lake grâce à des clusters de calcul optimisés par SQL\n",
    "- **réputation**: des clients prestigieux comme HP, Nasdac, Hotels.com ont mis en œuvre Databricks sur AWS pour mettre à disposition une plateforme d'analytique révolutionnaire répondant à tous les cas d'usage de l'analytique et de l'IA.\n",
    "\n",
    "Construire des modèles de ML est difficile et les mettre en production est encore plus difficile:\n",
    "- la **diversité des frameworks de ML** complique la gestion des environnements de ML\n",
    "- les **transferts entre équipes sont difficile**s en raison de la disparité des outils et des processus, de la préparation des données à l'expérimentation et à la production\n",
    "- la difficulté de suivre les expériences, les modèles, les dépendances et les artefacts rend **difficile la reproduction des résultats**\n",
    "- il y a des **risques liés à la sécurité et à la conformité**\n",
    "\n",
    "Le maintien de la qualité des données et de la précision des modèles au fil du temps ne sont que quelques-uns des défis à relever. Databricks va **rationaliser le développement ML**, de la préparation des données à l'entraînement et au déploiement des modèles, à **grande échelle** tout en permettant la collaboration:\n",
    "- **workspace**: un lieu central pour stocker et partager des notebooks, des expériences et des projets, avec un contrôle d'accès basé sur les rôles\n",
    "- **notebooks** collaboratifs: prise en charge de plusieurs langues (R, Python, SQL, Scala) et le versionning intégré permettent aux équipes de partager et d'itérer sur le code plus rapidement\n",
    "- **AutoML**: obtention de résultats plus rapides grâce à l'ajustement automatique des hyperparamètres et à la recherche de modèles avec les intégrations Hyperopt, Apache SparkTM et MLflow\n",
    "- **Experiments Tracking**: suivre automatiquement les expériences et utiliser les visualisations intégrées pour voir et comparer les résultats de milliers d'essais, sélectionner les paramètres, les mesures et identifier le meilleur essai.\n",
    "- **Model Registry**: enregistrer le modèle à mettre en production dans un seul endroit et gérer son cycle de vie de manière collaborative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0187c51b-76e7-4ae3-a22a-6462d945b4d7",
     "showTitle": false,
     "title": ""
    },
    "heading_collapsed": true
   },
   "source": [
    "## Monter un bucket AWS S3 sur Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3a0c97-0488-4991-ad37-e49385881aa8",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "**L'analyse des données directement dans un entrepôt de données peut s'avérer coûteuse**, c'est pourquoi les entreprises recherchent d'autres plateformes capables de stocker et de traiter tout ou partie de leurs données. \n",
    "\n",
    "L'un de ces outils est **Amazon S3** qui est un service en ligne qui offre un **stockage flexible aux entreprises**. Son contrôle d'accès granulaire, son chargement de métadonnées et d'autres caractéristiques de ce type en font le premier choix de tous les analystes de données. Aujourd'hui, les entreprises **transfèrent des informations de Databricks vers S3 afin d'utiliser un espace de stockage évolutif à un prix inférieur**.\n",
    "\n",
    "Pour monter un bucket AWS S3 sur Databricks, se référer au notebook suivant:\n",
    "- Mount_AWS_S3_Bucket.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ee0d9b-5758-4954-bd54-cf945fd47475",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36b8bb8-4b30-4c41-97ec-009d98b38d03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pyspark\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split, countDistinct\n",
    "\n",
    "# Python\n",
    "from PIL import Image\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Preprocessing images pour VGG16\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea3d0796-a2b2-4d49-b4d4-97f1538ddd6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1890876-9ab0-4318-a773-5fbcac36599b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dans cette partie, j'ai enregistré sur l'espace de stockage de Databricks quelques dossiers de fruits comportants chacun 5 images afin de tester le script sur un jeu de données réduit. J'avais uploadé des images sur Amazon S3 dans un premier temps mais cela revenait cher. Je profite de l'essai gratuit de Databricks pendant 14 jours pour l'élaboration du script avant de l'appliquer sur les données présentes sur S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c57870-f72d-4eff-8f39-2a3869bc3832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dossiers présents dans FileStore\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23dfa0d5-da75-4f86-a94d-284d6d4ac654",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Vérification du dossier où sont stockées les images\n",
    "dbutils.fs.ls(\"/FileStore/tables/data/test_S3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777815b0-c75e-4dae-9ecd-de677575f4b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Chargement des images au format image pour affichage\n",
    "df_img_avocado = spark.read.format(\"image\").load(\"/FileStore/tables/data/test_S3/Avocado/\")\n",
    "\n",
    "# Affichage des images\n",
    "display(df_img_avocado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e100fce-2efe-40c3-a58e-686beaa5b146",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Les différentes vues du fruit sont présentes dans des dossiers qui portent le nom du fruit. Nous allons **charger les données avec une extension.jpg** puis nous **rajouterons une colonne \"Label\"** correspondant au nom du dossier dans lequel se situent les images. Databricks recommande d'utiliser la source de données **fichier binaire** pour charger les données d'image dans le DataFrame Spark sous forme d'octets bruts.\\\n",
    "https://docs.databricks.com/external-data/image.html\n",
    "\n",
    "Databricks prend en charge les fichiers binaires et convertit chacun d’eux en un enregistrement unique qui contient le **contenu brut et les métadonnées du fichier**. La source de données de fichier binaire produit un DataFrame avec les colonnes suivantes et éventuellement des colonnes de partition :\n",
    "- **path (StringType)** : Chemin d'accès au fichier.\n",
    "- **modificationTime (TimestampType)** : Heure de dernière modification de l'événement. Dans certaines implémentations Hadoop FileSystem, ce paramètre peut ne pas être disponible et la valeur est définie sur une valeur par défaut.\n",
    "- **length (LongType)** : Longueur du fichier en octets.\n",
    "- **content (BinaryType)** : Contenu du fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b10567-b678-42d8-aad5-35c2399e3be6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Chemin d'accès des données\n",
    "path_data = \"/FileStore/tables/data/test_S3/\"\n",
    "\n",
    "# lecture des fichiers jpg de manière récursive à partir du répertoire \n",
    "# d'entrée en ignorant la détection de la partition\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31d9a07-4d54-4c73-8457-125244a7ddff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(images.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8fa749e-1a25-4bbe-91f5-1a11ad950721",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5 premières lignes du dataframe issu de la source de données de fichier binaire\n",
    "images.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ac9900-5432-4194-bfad-0e7d59b724dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nous aurons besoin du **chemin d'accès** aux images (colonne **path**) ainsi que d'une colonne contenant les **labels** de chaque image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19fd426-3946-4e92-85a1-d01e92c3fe03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Création colonne label\n",
    "df_images = images.withColumn('label', element_at(split(images['path'], '/'),-2)) \n",
    "print(df_images.printSchema())\n",
    "\n",
    "# Affichage des 5 premières lignes\n",
    "df_images.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8255b88a-4c8d-4ba4-9e52-e330c95fd270",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Quelques statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6eb61e-8372-4ba0-82d4-15250e7d24e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nombre d'images par label\n",
    "df_images.groupBy(\"label\").agg(countDistinct('path')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc5b5524-35b3-4e25-8a2d-4f41621cc708",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcul du nombre de labels\n",
    "df_images.select(countDistinct(\"label\")).show()\n",
    "#images.select('label').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32b2f1f2-2f7a-4d15-903a-27614117dfed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcul du nombre d'images\n",
    "df_images.select(countDistinct(\"path\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8534acc-6885-4aaa-8d16-2a6ec7b93637",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Preprocessing des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56543626-6026-43dc-b089-1321fc3cbfa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Une série de modèles très performants ont été développés pour la classification d'images et démontrés lors du concours annuel ImageNet.\n",
    "Ces modèles peuvent être utilisés comme base pour l'apprentissage par transfert dans les applications de vision par ordinateur pour plusieurs raisons:\n",
    "- Les modèles ont appris à détecter les **caractéristiques génériques des photographies**, étant donné qu'ils ont été **entraînés sur plus de 14 000 000 d'images pour 1 000 catégories**.\n",
    "- **Performances de pointe** : Les modèles ont atteint une performance de pointe et restent efficaces dans la tâche spécifique de reconnaissance d'images pour laquelle ils ont été développés.\n",
    "- **Facilité d'accès** : Les poids des modèles sont fournis sous forme de fichiers téléchargeables gratuitement et de nombreuses bibliothèques fournissent des API pratiques pour télécharger et utiliser les modèles directement.\n",
    "\n",
    "Il existe une douzaine de modèles de reconnaissance d'images très performants qui peuvent être téléchargés et utilisés comme base pour la reconnaissance d'images et les tâches connexes de vision par ordinateur.\n",
    "\n",
    "Trois des modèles les plus populaires sont les suivants :\n",
    "- VGG (par exemple, VGG16 ou VGG19)\n",
    "- GoogLeNet (par ex. InceptionV3)\n",
    "- Réseau résiduel (par exemple, ResNet50)\n",
    "\n",
    "**Keras** donne accès à un certain nombre de modèles pré-entraînés très performants qui ont été développés pour des tâches de reconnaissance d'images.\n",
    "\n",
    "Ils sont disponibles via l'API d'applications et comprennent des fonctions permettant de **charger un modèle avec ou sans les poids pré-entraînés**, et de **préparer les données** d'une manière qu'un modèle donné peut attendre (par exemple, la mise à l'échelle de la taille et des valeurs des pixels)\\\n",
    "https://keras.io/api/applications/vgg/\n",
    "\n",
    "Le modèle pré-entraîné peut être utilisé comme un programme autonome pour extraire des caractéristiques de nouvelles photographies.\n",
    "\n",
    "Ici nous allons exploiter le réseau de neurones pré-entrainé **VGG16 pour extraire les features** c’est-à-dire se servir des features du réseau pour représenter les images du nouveau problème. La dernière couche fully-connected sera retirée car nous ne souhaitons pas à ce stade effectuer la classification.\n",
    "\n",
    "La taille d'entrée par défaut des images pour ce modèle est **224x224**. Chaque application Keras attend un type spécifique de prétraitement des entrées. Pour VGG16:\n",
    "- conversion des images d'entrée de RGB à BGR\n",
    "- centrage à zéro de chaque canal de couleur par rapport à l'ensemble de données ImageNet, sans mise à l'échelle \\\n",
    "Ce preprocessing est effectué par la fonction tf.keras.applications.vgg16.preprocess_input\n",
    "\n",
    "Nous allons charger chaque photo au format binaire, télécharger et préparer le modèle comme un modèle d'extraction de caractéristiques (cad enlever la dernière couche) puis extraire ces caractéristiques de la photo chargée à l'aide d'un UDF pandas Scalar Iterator.\n",
    "\n",
    "Nous allons reprendre le script de la documentation issue de Databricks en l'adaptant à un modèle VGG16: \\\n",
    "https://docs.databricks.com/machine-learning/preprocess-data/transfer-learning-tensorflow.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4964bffd-9416-4be8-9111-22df5c7c1dc2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Préparation du modèle\n",
    "\n",
    "Les workers Spark ont besoin d'accéder au modèle et à ses poids. Le modèle sera chargé dans le pilote Spark puis les poids seront ensuite diffusés aux workers. \\\n",
    "Au lieu de distribuer ces informations avec chaque tâche sur le réseau (surcharge et perte de temps), les variables de diffusion sont utilisées. Ce sont des **variables partagées en lecture seule** qui sont **mises en cache et disponibles sur tous les nœuds du cluster** afin que les tâches puissent y accéder ou les utiliser. PySpark distribue donc les variables de diffusion aux travailleurs à l'aide d'algorithmes de diffusion efficaces afin de **réduire les coûts de communication au lieu d'envoyer ces données avec chaque tâche**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1808c9f-e85c-4ac8-9647-1c58e9d768a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Modèle VGG16\n",
    "train_time = time.time()\n",
    "# Chargement du modèle VGG16 et suppression de la dernière couche\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "# Vérification de la suppression de la dernière couche (predictions (Dense & Fully Connected))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa83b084-a87e-432b-8c52-c3726eb8fafe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Broadcasting des poids (on récupère les poids du modèle cad imagenet => on passe en C plus bas niveau donc plus rapide)\n",
    "bc_model_weights = sc.broadcast(model.get_weights())\n",
    "\n",
    "def model_fn():\n",
    "    \"\"\" Modèle VGG16 avec suppression de la dernière couche et broadcasting des poids \"\"\"\n",
    "    model = VGG16(weights=None, include_top=False)\n",
    "    model.set_weights(bc_model_weights.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c7b7fe-9a84-4a93-ba53-e2b2892434c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Définition du processus de chargement des images et application de leur featurisation à travers l'utilisation de pandas UDF\n",
    "\n",
    "Le nouveau Scalar Iterator pandas UDF est utilisé afin **d'amortir le coût du chargement de grands modèles sur les workers**. Il permet des **opérations vectorisées** qui peuvent augmenter les performances jusqu'à 100 fois par rapport aux UDF Python ligne par ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a46ffe2-41c0-4e76-9597-e52fc2a5b2a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\" Preprocessing des images conservées sous forme \n",
    "    d'octets dans un cache en mémoire.\n",
    "    \n",
    "    Arguments:\n",
    "    --------------------------------\n",
    "    content: : image au format binaire\n",
    "    \n",
    "    return:\n",
    "    --------------------------------\n",
    "    Image redimensionnée et preprocessée au format array \"\"\"\n",
    "    \n",
    "    # Taille d'entrée des images pour le VGG16: 224 * 224\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    # Format array\n",
    "    arr = img_to_array(img)\n",
    "    # Centrage à zéro de chaque canal de couleur par rapport à l'ensemble de données ImageNet, sans mise à l'échelle \n",
    "    return preprocess_input(arr)\n",
    "\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\" Extraction des caractéristiques des images au format Array\n",
    "    par le modèle indiqué en entrée.\n",
    "    \n",
    "    Arguments:\n",
    "    --------------------------------\n",
    "    model: modèle d'extraction des features\n",
    "    content_series: images redimensionnées et preprocessées au format array\n",
    "    \n",
    "    return: \n",
    "    --------------------------------\n",
    "    caractéristiques de l'image au format pd.Series \"\"\"\n",
    "    \n",
    "    # Parallélisation des calculs pour le preprocessing des images\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    # Extraction des features\n",
    "    preds = model.predict(input)\n",
    "    # Pour certaines couches, les caractéristiques de sortie seront des tenseurs multidimensionnels.\n",
    "    # Vectorisation des features tensors pour faciliter le stockage dans les Spark DataFrames\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2140beb-71ac-4b28-b741-c2b56f9be463",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Méthode renvoie une colonne Spark DataFrame de type ArrayType(FloatType).\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    \"\"\" Fonction qui charge le modèle VGG16 avec suppression de la dernière couche et broadcasting des poids\n",
    "    puis applique l'extraction de features de manière itérative aux images redimensionnées et preprocessées \n",
    "    au format array\n",
    "    \n",
    "    Arguments:\n",
    "    --------------------------------\n",
    "    content_series_iter: itérateur des images redimensionnées et preprocessées au format array\n",
    "    \n",
    "    return: \n",
    "    --------------------------------\n",
    "    caractéristiques de l'image au format pd.Series \"\"\"\n",
    "\n",
    "  # Avec les Pandas UDF Scalar Iterator, nous pouvons charger le modèle une fois et le réutiliser ensuite\n",
    "  # pour plusieurs lots de données.  Cela permet d'amortir les frais généraux liés au chargement de gros modèles\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda2d064-b93f-463a-9c04-ae7b09335885",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Exécutions des actions d'extractions de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b081d1e0-fd6b-49cd-8937-5b82b4120f92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Les Pandas UDF sur des enregistrements de grande taille peuvent rencontrer des erreurs de type Out Of Memory.\n",
    "# Si c'est le cas, tenter de réduire la taille de `maxRecordsPerBatch`:\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc4e6c99-f992-40ce-8537-791c67fa7d77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_df = df_images.repartition(24).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e3d7e3f-df70-47a9-b437-cbffb324d58d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Affichage du dataframe partitionné par hachage (16 partitions)\n",
    "features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef8e4002-cdb5-4b66-82f9-d1b06046207a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Réduction de dimension par ACP\n",
    "\n",
    "Nous allons réaliser une ACP qui va permettre de **créer des features décorrélées entre elles**, de **diminuer leur dimension** tout en gardant un **niveau de variance expliquée élevée**. \\\n",
    "La réduction de dimension sera effectuée à l'aide de la documentation ci-dessous: \\\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html \\\n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/ml/pca_example.py \\\n",
    "https://people.duke.edu/~ccc14/sta-663-2016/21D_Spark_MLib.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8439c1-114c-4b19-8195-4773c766e1cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ACP\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "580f464e-5066-424d-a178-f48580a98861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extraction des features individuelles et assemblage en vecteur\n",
    "# https://stackoverflow.com/questions/39025707/how-to-convert-arraytype-to-densevector-in-pyspark-dataframe\n",
    "\n",
    "n = 22 # Size of features\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"features[{0}]\".format(i) for i in range(n)], \n",
    "    outputCol=\"features_vector\")\n",
    "\n",
    "output = assembler.transform(features_df.select(\n",
    "    \"*\", *(features_df[\"features\"].getItem(i) for i in range(n))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2599f79e-e22d-4f3b-b127-46dd5d8d051b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardisation (moyenne = 0 et écart type = 1)\n",
    "scaler = StandardScaler(withMean=True, withStd=True,\n",
    "                        inputCol='features_vector',\n",
    "                        outputCol='std_features')\n",
    "\n",
    "model = scaler.fit(output)\n",
    "output = model.transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b610a28-ee70-4d6a-9018-089bc2532474",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ACP\n",
    "pca = PCA(k=3, inputCol=\"std_features\", outputCol=\"pcaFeatures\")\n",
    "pca = pca.fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e354153f-eb56-4e80-81aa-069c07b1262c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_scree_plot(pca):\n",
    "    \"\"\"Fonction qui affiche l'éboulis des valeurs propres.\n",
    "\n",
    "    Arguments:\n",
    "    --------------------------------\n",
    "    pca: acp, obligatoire\n",
    "    \n",
    "    return:\n",
    "    --------------------------------\n",
    "    None \"\"\"\n",
    "    \n",
    "    scree = pca.explainedVariance*100\n",
    "    plt.bar(np.arange(len(scree))+1, scree)\n",
    "    plt.plot(np.arange(len(scree))+1, scree.cumsum(), c=\"red\", marker='o')\n",
    "    plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\")\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f27c326-f248-4a92-869b-ec6d968c904b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "display_scree_plot(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b28bd41d-d19a-4d84-8e5f-7c3d3e4eaefb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Le graphique montre la quantité de variance capturée (sur l'axe des y) en fonction du nombre de composantes que nous incluons (sur l'axe des x). Une règle empirique consiste à préserver environ 80 % de la variance. Il faudrait donc garder **250 composantes**.\n",
    "\n",
    "Nous effectuons l'ACP avec le nombre de composantes choisi ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2429f0b-0b85-4bdc-9086-e350702b12c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b5dfb5d-5992-49bf-8ab0-f98c2c25edf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75fc3177-c625-4121-9666-de8bcb6b0b7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e00491f-9258-4d0d-b193-b9c8b7d8838a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform 60 features into MMlib vectors\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=\"features\",\n",
    "    outputCol=\"features_vectors\")\n",
    "\n",
    "output = assembler.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf2825cb-a91a-44db-aeb7-a7f3acb2c59a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cf767df-2812-40df-bc90-aa3cc352de03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b88d59d0-848e-4165-9d5a-6fdce2b3e33d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09a5272c-8f48-4d2c-8a4f-986af8b5f16c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Création d'un dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2832393e-6b8e-4747-b351-ec0ede2cdf3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(features_df.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9264f32-4536-46ee-beb0-8088d2af1af5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c5b197d-e0fc-48f8-9c78-2cf990e9e231",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31cd9499-a52b-4568-9e20-59bbe34e2ab5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duration_vgg16 = np.round(time.time() - train_time,0)\n",
    "print(f\"Temps de traitement: {duration_vgg16} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00550101-e41c-4ff7-8716-6b23b1efaf38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PCA avec Pyspark + envoi sur S3 (dbutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e020e64c-deab-4a3e-865d-8d4ac19e40f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(\"dbfs:/ml/tmp/flower_photos_features\")\n",
    "\n",
    "\n",
    "dbutils.fs.ls(\"/FileStore/tables/data/test_S3/\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GROSCHENE_Emilie_1_notebook_databricks_062023",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "projet8",
   "language": "python",
   "name": "projet8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
