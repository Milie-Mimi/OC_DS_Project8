{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e58e61-96c4-471e-a3de-5d9236c3dcfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the credential file is uploaded\n",
    "\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff287d9f-600f-475e-a824-ba3fcce47172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_type = \"csv\"\n",
    "first_row_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/databricks_accessKeys.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4717fb5b-0d72-468f-b2a6-7a6ac44a14cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To send keys to AWS\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Collect access and secret key from spark dataframe (dbfs:/FileStore/tables/)\n",
    "ACCESS_KEY = aws_keys_df.select('Access_key_ID').collect()[0]['Access_key_ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret_access_key').collect()[0]['Secret_access_key']\n",
    "\n",
    "# Encode secret key (safe=\"\" means every character in the secret key is encoded)\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(SECRET_KEY,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b9e721-6299-4a7f-a8c1-70adc7b89793",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount the S3 bucket\n",
    "\n",
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"fruits-pictures\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/fruits-pictures\"\n",
    "# Source url\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69906939-7c7e-4f39-956f-d017d28eeea8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read data from the mounted S3 bucket\n",
    "\n",
    "# Check if the AWS S3 bucket was mounted successfully\n",
    "# %fs ls \"/mnt/fruits-pictures/\"\n",
    "display(dbutils.fs.ls(\"/mnt/fruits-pictures/test_S3/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b050198-79ba-434e-8f9c-118e98524d09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"image\").option(\"dropInvalid\", true).load(\"dbfs:/mnt/fruits-pictures/test_S3/Apricot/3_100.jpg\")\n",
    "df.select(\"image.origin\", \"image.width\", \"image.height\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2232f6b-1f00-4e66-818e-e5cd995dea77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4260875087482885,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Mount_AWS_S3_Bucket",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
